以下是手把手搭建《拳皇》AI训练项目的分步指南：

---

### **1. 环境准备**
#### 1.1 基础依赖安装
```bash
# Python环境（推荐3.8+）
conda create -n kof python=3.8
conda activate kof

# 核心库
pip install gym retro==3.0.0 stable-baselines3 torch torchvision tqdm opencv-python pygame numpy pandas
```

#### 1.2 游戏模拟器配置
- **MAME安装**：
  - Linux: `sudo apt-get install mame`
  - Windows: 从[MAME官网](https://www.mame.net/)下载安装包
- **ROM文件准备**：
  - 将《拳皇97》ROM文件（如`kof97.zip`）放入`roms/kof97`目录
  - 确保文件名与代码中`roms`目录结构一致

---

### **2. 项目目录初始化**
```bash
mkdir kof_ai_project && cd kof_ai_project
mkdir -p king_of_fighter/ppo roms/kof97 scripts trained_models
```

#### 关键文件结构
```
.
├── king_of_fighter
│   ├── actions.py       # 动作枚举定义
│   ├── address.py       # 内存地址映射
│   ├── kof_environment.py # 自定义Gym环境
│   └── ppo             # PPO算法实现
│       ├── ppo.py
│       ├── resnet_policy.py
│       └── ...
├── roms
│   └── kof97            # ROM文件存放
├── scripts
│   ├── train.sh         # 训练脚本
│   └── run.sh           # 测试脚本
└── ...
```

---

### **3. 核心代码实现**
#### 3.1 定义游戏动作（`actions.py`）
```python
from enum import Enum
from MAMEToolkit.emulator import Action

class Actions(Enum):
    COIN_P1 = Action(":edge:joy:JOY1", "Insert Coin P1")
    P1_UP = Action(":edge:joy:JOY1", "P1 Up")
    P1_PUNCH = Action(":edge:joy:JOY1", "P1 Button 1")
    # 其他动作定义...
```

#### 3.2 配置内存地址（`address.py`）
```python
from MAMEToolkit.emulator import Address

def setup_memory_addresses():
    return {
        "time": Address(0x203C, "u8"),
        "healthP1": Address(0x3000, "u16"),
        "healthP2": Address(0x3200, "u16")
    }
```

#### 3.3 实现Gym环境（`kof_environment.py`）
```python
import gym
from gym import spaces
import numpy as np
from MAMEToolkit.emulator import Emulator

class KoFEnvironment(gym.Env):
    def __init__(self, rom_path):
        self.emulator = Emulator("kof97", rom_path)
        self.observation_space = spaces.Box(low=0, high=255, shape=(84,84,3))
        self.action_space = spaces.Discrete(10)  # 10个动作
        
    def reset(self):
        self.emulator.reset()
        return self._get_obs()
        
    def step(self, action):
        self._take_action(action)
        obs = self._get_obs()
        reward = self._calculate_reward()
        done = self._check_done()
        return obs, reward, done, {}
```

---

### **4. 训练流程实现**
#### 4.1 训练脚本（`scripts/train.sh`）
```bash
#!/bin/bash
python -m king_of_fighter.baseline \
  --role kyo \
  --total_steps 100000 \
  --batch_size 256 \
  --save_dir trained_models
```

#### 4.2 PPO策略网络（`ppo/resnet_policy.py`）
```python
import torch.nn as nn
from stable_baselines3.common.policies import ActorCriticPolicy

class ActorCriticResnetPolicy(ActorCriticPolicy):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.features_extractor = ResNet18()  # 自定义ResNet
        self.mlp_extractor = MlpExtractor(...)
```

---

### **5. 训练与测试**
#### 5.1 启动训练
```bash
chmod +x scripts/train.sh
./scripts/train.sh
```

#### 5.2 实时测试（`debug.py`）
```python
env = KoFEnvironment()
model = PPO.load("trained_models/ppo_kyo")
obs = env.reset()
while True:
    action, _ = model.predict(obs)
    obs, reward, done, _ = env.step(action)
    if done:
        break
```

---

### **6. 关键调试点**
1. **ROM路径问题**：确保`roms/kof97`中的文件与代码中的路径匹配
2. **依赖版本冲突**：使用`pip freeze > requirements.txt`管理版本
3. **显存不足**：降低`batch_size`或使用`--device cpu`
4. **动作响应延迟**：在环境封装中调整`frame_skip`参数

---

通过以上步骤，可逐步搭建完整的《拳皇》AI训练系统。建议先从基础环境验证开始，逐步添加强化学习组件，最后进行端到端训练。